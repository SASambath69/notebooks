{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analyse des Messages d'un Groupe WhatsApp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrGGbIjhpSHwFb9FqzQ2mk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SASambath69/notebooks/blob/main/Analyse_des_Messages_d'un_Groupe_WhatsApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35_cV6K-ut2y"
      },
      "source": [
        "# Objectifs\n",
        "\n",
        "Le présent notebook a été réalisé pour effectuer une analyse des discussions d'un groupe WhatsApp. Les informations concernant les expéditeurs seront anonymisées. Les objectifs seront les suivants :\n",
        "\n",
        "*    Combien de messages ont été envoyés dans ce groupe au total par année ? Puis par mois ?\n",
        "*    Qui sont les contributeurs les plus actifs ?\n",
        "*    A quel moment de la journée et quel jour de la semaine il y a le plus de messages ?\n",
        "*    Quels sont les mots les plus utilisés ?\n",
        "*    Quels sont les emojis les plus présents ?\n",
        "\n",
        "# Cheminement\n",
        "\n",
        "1.    Récupération du fichier de sauvegarde de la discussion (.txt)\n",
        "2.    Retraitement du fichier et création de DataFrames\n",
        "3.    Sauvegarde des DataFrames dans un fichier Excel\n",
        "4.    Mise en place d'un rapport Power BI présentant les informations requises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6odBeb0mKQ3"
      },
      "source": [
        "# Import des Librairies\n",
        "\n",
        "Petites précisions sur quelques librairies utilisées :\n",
        "\n",
        "*    nltk : pour l'analyse des mots contenus dans les messages\n",
        "*    string : servira pour retirer les ponctuations lors de l'analyse des mots\n",
        "*    emoji : pour l'analyse des emojis les plus utilsés\n",
        "*    InteractiveShell : pour afficher tous les outputs d'un bloc de code, et ne pas se limiter au dernier output\n",
        "*    drive : pour me connecter à mon Google Drive et récupérer le fichier txt correspondant à la sauvegarde de la discussion WhatsApp\n",
        "*    google.colab files : pour télécharger le fichier Excel contenant les DataFrames qui serviront de base à la mise en place du Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaMkU75Xj6Ls"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('popular')\n",
        "import string\n",
        "!pip install emoji\n",
        "import emoji\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgUHEcTskKSX"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDJogjBwsVx9"
      },
      "source": [
        "# Import et Retraitement du fichier contenant les discussions\n",
        "\n",
        "Par défaut, les discussions vont être retranscris dans un DataFrame à 1 colonne. Les retraitements et travaux seront les suivants :\n",
        "\n",
        "1.    Identification des lignes qui correspondent à des suites de messages. En effet, selon la longueur des messages, ceux-ci peuvent apparaître sur plusieurs lignes du DataFrame. L'objectif est ainsi de les repérer selon différents moyens (longueur du texte inférieur à un certain nombre ou absence de date dans les 1ers caractères du message) puis de les replacer à la suite des messages dont ils correspondent\n",
        "2.    Création d'une colonne contenant la date et heure de l'envoi du message\n",
        "3.    Création d'une colonne contenant le nom de l'expéditeur et anonymisation\n",
        "4.    Création de 2 colonnes correspondant au jour de la semaine (de 1 à 7) et à l'heure de la journée (de 0 à 23), puis pivot table pour afficher le nombre de messages envoyés par heure de la jour et par jour de la semaine\n",
        "5.    Création d'une colonne contenant le message\n",
        "6.    Identification des mots les plus utilisés (ajout de certains mots, expressions, caractères à la liste des stopwords). Pour les messages contenant des médias, ceux-ci apparaissent avec la mention \"<Médias omis>\". Je décide de ne pas en tenir compte\n",
        "7.    Identification des emojis les plus utilisés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6aQJX4iLVPj"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Discussion WhatsApp avec Le Verty Link Brignairot.txt', delimiter = \"\\t\", header = None, names = ['text'])\n",
        "\n",
        "# ------------------------------ Identification des lignes correspondant à des suites de message et replacement de ces derniers à la suite des messages auxquels ils appartiennent\n",
        "\n",
        "df['follow_message'] = df['text'].apply(lambda x: True if len(x) < 10 else (True if x[2] != '/' or x[5] != '/' else False))\n",
        "df['text_def'] = df['text']\n",
        "for i in range(len(df['text'])):\n",
        "  if i != len(df['text'])-1 and df['follow_message'][i] == False and df['follow_message'][i+1] == False:\n",
        "    pass\n",
        "  elif df['follow_message'][i] == True:\n",
        "    start = i\n",
        "    while df['follow_message'][start] == True:\n",
        "      df['text_def'][i-1] = df['text_def'][i-1] + ' ' + df['text_def'][start]\n",
        "      start += 1\n",
        "\n",
        "df = df.loc[df['follow_message'] == False].drop(columns=['text','follow_message'])\n",
        "\n",
        "# ------------------------------ Extraction des dates et heures\n",
        "\n",
        "df[['datetime_str','no_date']] = df[\"text_def\"].str.split(' - ', 1, expand=True)\n",
        "df['datetime_str'] = df['datetime_str'].str.replace(' à', '')\n",
        "df['datetime_str'] = pd.to_datetime(df['datetime_str'])\n",
        "\n",
        "# ------------------------------ Extraction des expéditeurs\n",
        "\n",
        "df = df.loc[df['no_date'].str.contains(':')==True]\n",
        "df[['sender','text_message']] = df['no_date'].str.split(': ', 1, expand=True)\n",
        "\n",
        "# ------------------------------ Anonymisation\n",
        "\n",
        "senders = list(df['sender'].unique())\n",
        "anonyme = []\n",
        "for i in range(len(df['sender'].unique())):\n",
        "  anonyme.append('sender_{}'.format(i+1))\n",
        "df['sender'] = df['sender'].replace(senders, anonyme)\n",
        "\n",
        "# ------------------------------ Stopwords\n",
        "\n",
        "stop_words = list(stopwords.words('french'))\n",
        "stop_words.extend(['a','’','je','ça','ca','c','j','et','...','va',\"c'est\",'vais','fait',\"''\",'``',\"j'ai\",'^^'])\n",
        "\n",
        "# ------------------------------ Mots les plus utilisés par expéditeur (tout le monde)\n",
        "\n",
        "df_by_sender = pd.DataFrame()\n",
        "\n",
        "# Certains messages correspondent en réalité à des médias et sont inscrits \"<Médias omis>\". On n'en tient pas compte pour la suite\n",
        "\n",
        "for i in range(len(df['sender'].unique())):\n",
        "  sender = df['sender'].value_counts().index[i]\n",
        "  df_sender = df.loc[df['sender'] == sender].loc[df['text_message'].str.lower().str.contains('médias omis')==False]\n",
        "  all_words_sender = ''\n",
        "  for j in df_sender['text_message']:\n",
        "    all_words_sender = all_words_sender + ' ' + j\n",
        "  df_words_sender = pd.DataFrame({'sender' : sender, 'words' : word_tokenize(all_words_sender)})\n",
        "  df_words_sender['no_stop_words'] = df_words_sender['words'].apply(lambda x: x.lower() if x.lower() not in stop_words and x.lower() not in list(string.punctuation) and x not in emoji.UNICODE_EMOJI['en'] else np.NaN)\n",
        "  df_words_sender_top = pd.DataFrame({'sender' : sender, 'words' : list(df_words_sender['no_stop_words'].value_counts().head(20).index), 'count' : list(df_words_sender['no_stop_words'].value_counts().head(20).values)})\n",
        "\n",
        "  df_by_sender = pd.concat([df_by_sender, df_words_sender_top], axis = 0)\n",
        "\n",
        "del df_sender\n",
        "del df_words_sender\n",
        "del df_words_sender_top\n",
        "\n",
        "# ------------------------------ Emojis les plus utilisés par expéditeur (tout le monde)\n",
        "\n",
        "df_emo_by_sender = pd.DataFrame()\n",
        "\n",
        "# Certains messages correspondent en réalité à des médias et sont inscrits \"<Médias omis>\". On n'en tient pas compte pour la suite\n",
        "\n",
        "for i in range(len(df['sender'].unique())):\n",
        "  sender = df['sender'].value_counts().index[i]\n",
        "  df_sender = df.loc[df['sender'] == sender].loc[df['text_message'].str.lower().str.contains('médias omis')==False]\n",
        "  all_words_sender = ''\n",
        "  for j in df_sender['text_message']:\n",
        "    all_words_sender = all_words_sender + ' ' + j\n",
        "  df_words_sender = pd.DataFrame({'sender' : sender, 'words' : word_tokenize(all_words_sender)})\n",
        "  df_words_sender['emoji'] = df_words_sender['words'].apply(lambda x: x if x in emoji.UNICODE_EMOJI['en'] else np.NaN)\n",
        "  df_words_sender_top = pd.DataFrame({'sender' : sender, 'words' : list(df_words_sender['emoji'].value_counts().head(20).index), 'count' : list(df_words_sender['emoji'].value_counts().head(20).values)})\n",
        "\n",
        "  df_emo_by_sender = pd.concat([df_emo_by_sender, df_words_sender_top], axis = 0)\n",
        "\n",
        "del df_sender\n",
        "del df_words_sender\n",
        "del df_words_sender_top"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGuqvuQRZdeL"
      },
      "source": [
        "# Sauvegarde des DataFrames dans un fichier Excel et téléchargement de ce fichier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsLF_dzOZaKV"
      },
      "source": [
        "with pd.ExcelWriter('whatsapp_{}.xlsx'.format(str(datetime.date.today()))) as writer:\r\n",
        "  df[['datetime_str','sender']].to_excel(writer, sheet_name='All')\r\n",
        "  df_by_sender.to_excel(writer, sheet_name='Words')\r\n",
        "  df_emo_by_sender.to_excel(writer, sheet_name='Emoji')\r\n",
        "\r\n",
        "files.download('whatsapp_{}.xlsx'.format(str(datetime.date.today())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7r2V3pbSZj"
      },
      "source": [
        "# Rapport Power BI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNmMX-6ad18G"
      },
      "source": [
        "[Lien vers le rapport](https://app.powerbi.com/view?r=eyJrIjoiZjUyN2Y4YjUtYTZiMi00YjZjLWE1NjItMzZlZWNiNjZkM2MyIiwidCI6IjE0NTJmNzE3LTQ5MTItNDE1Yi1hZjg1LWQ3Njc5YWM0MWQwNiJ9)"
      ]
    }
  ]
}